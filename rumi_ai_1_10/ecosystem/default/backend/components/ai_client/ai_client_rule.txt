# `ai_client/ai_client_rule.txt`

```text
================================================================================
                      AI CLIENT IMPLEMENTATION RULES
                           ai_client_rule.txt
================================================================================

このファイルは、ai_clientフォルダ内にAIプロバイダーを実装する際のルールを定義します。

--------------------------------------------------------------------------------
1. ディレクトリ構造
--------------------------------------------------------------------------------

ai_client/
├── ai_client_loader.py               # クライアント読み込みエンジン（変更禁止）
├── ai_client_dependency_manager.py   # 依存関係管理（変更禁止）
├── ai_client_rule.txt                # このファイル
│
└── [provider_name]/                  # 個別プロバイダーフォルダ
    ├── .venv/                        # 自動生成される仮想環境
    ├── requirements.txt              # 依存ライブラリ（必須）
    ├── [provider_name]_client.py     # エントリーポイント（必須）
    └── ai_profile/                   # モデルプロファイル格納フォルダ
        ├── model-id-1.json
        ├── model-id-2.json
        └── ...

--------------------------------------------------------------------------------
2. ファイル命名規則
--------------------------------------------------------------------------------

- プロバイダーフォルダ名: 小文字英数字とアンダースコアのみ
  例: gemini, openai, anthropic, azure_openai

- エントリーポイント: `[provider_name]_client.py`
  例: gemini_client.py, openai_client.py, anthropic_client.py

- クラス名: `[ProviderName]Client`（キャメルケース）
  例: GeminiClient, OpenAIClient, AnthropicClient

- プロファイルフォルダ: `ai_profile/`（固定名）

- プロファイルファイル: `[model-id].json`
  例: gemini-2.5-pro.json, gpt-4o.json, claude-3-opus.json

--------------------------------------------------------------------------------
3. 必須クラスメソッド
--------------------------------------------------------------------------------

各クライアントクラスは以下のメソッドを必ず実装すること:

```python
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional
from threading import Event


class [ProviderName]Client:
    """
    [プロバイダー名]のAIクライアント実装
    """
    
    # =========================================
    # 静的メソッド（必須）
    # =========================================
    
    @staticmethod
    def get_provider_name() -> str:
        """
        プロバイダー名を返す。
        フォルダ名と一致させること。
        
        Returns:
            str: プロバイダー名（例: "gemini", "openai"）
        """
        return "provider_name"
    
    @staticmethod
    def get_profile_dir() -> Path:
        """
        プロファイルディレクトリのパスを返す。
        
        Returns:
            Path: ai_profile/ディレクトリの絶対パス
        """
        return Path(__file__).parent / "ai_profile"
    
    # =========================================
    # コンストラクタ（必須）
    # =========================================
    
    def __init__(self):
        """
        クライアントを初期化する。
        
        APIキーは環境変数から自動取得すること。
        環境変数名: [PROVIDER_NAME]_API_KEY
        例: GEMINI_API_KEY, OPENAI_API_KEY
        
        APIキーが見つからない場合はValueErrorを発生させること。
        """
        pass
    
    # =========================================
    # リクエスト送信メソッド（必須）
    # =========================================
    
    def send_request(
        self,
        model_id: str,
        history: Dict,
        current_text_input: str,
        current_file_paths: List[str],
        temperature: float = 0.7,
        thinking_budget: Optional[int] = None,
        tools: Optional[List] = None,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> Any:
        """
        非ストリーミングでリクエストを送信する。
        
        Args:
            model_id: 使用するモデルのID
            history: 標準形式の会話履歴（後述）
            current_text_input: 現在のユーザー入力テキスト
            current_file_paths: 添付ファイルのパスリスト
            temperature: 生成温度（0.0-2.0）
            thinking_budget: 思考トークン予算（対応モデルのみ）
            tools: Function Calling用ツール定義
            system_prompt: システムプロンプト
            **kwargs: プロバイダー固有のパラメータ
        
        Returns:
            Any: プロバイダー固有のレスポンスオブジェクト
        
        Raises:
            ValueError: パラメータが不正な場合
            APIError: API呼び出しに失敗した場合
        """
        pass
    
    def send_request_stream(
        self,
        model_id: str,
        history: Dict,
        current_text_input: str,
        current_file_paths: List[str],
        temperature: float = 0.7,
        thinking_budget: Optional[int] = None,
        tools: Optional[List] = None,
        abort_signal: Optional[Event] = None,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> Iterator:
        """
        ストリーミングでリクエストを送信する。
        
        Args:
            model_id: 使用するモデルのID
            history: 標準形式の会話履歴
            current_text_input: 現在のユーザー入力テキスト
            current_file_paths: 添付ファイルのパスリスト
            temperature: 生成温度（0.0-2.0）
            thinking_budget: 思考トークン予算（対応モデルのみ）
            tools: Function Calling用ツール定義
            abort_signal: 中断シグナル（threading.Event）
            system_prompt: システムプロンプト
            **kwargs: プロバイダー固有のパラメータ
        
        Yields:
            dict: イベントオブジェクト（後述の形式）
        
        Raises:
            ValueError: パラメータが不正な場合
            APIError: API呼び出しに失敗した場合
        """
        pass
    
    # =========================================
    # レスポンス処理メソッド（必須）
    # =========================================
    
    def extract_response_text(self, response: Any) -> str:
        """
        レスポンスオブジェクトからテキストを抽出する。
        
        Args:
            response: send_request()の戻り値
        
        Returns:
            str: 抽出されたテキスト
        """
        pass
    
    # =========================================
    # Function Calling関連メソッド（必須）
    # =========================================
    
    def handle_function_calls(
        self,
        response: Any,
        model_id: str,
        history: Dict,
        context: Dict
    ) -> tuple:
        """
        Function Callを処理する。
        
        Args:
            response: AI応答オブジェクト
            model_id: モデルID
            history: 標準形式の会話履歴
            context: 実行コンテキスト（後述）
        
        Returns:
            tuple: (最終レスポンス, 実行結果リスト)
        """
        pass
    
    def handle_function_calls_stream(
        self,
        stream_response: Iterator,
        model_id: str,
        history: Dict,
        context: Dict
    ) -> Iterator:
        """
        ストリーミング版のFunction Call処理。
        
        Args:
            stream_response: ストリームイテレータ
            model_id: モデルID
            history: 標準形式の会話履歴
            context: 実行コンテキスト
        
        Yields:
            dict: 処理済みイベントオブジェクト
        """
        pass
    
    # =========================================
    # オプションメソッド
    # =========================================
    
    def abort_streaming(self) -> None:
        """
        現在のストリーミングを強制停止する。
        ストリーミング対応の場合は実装すること。
        """
        pass
    
    def get_masked_api_key(self) -> str:
        """
        マスクされたAPIキーを返す（デバッグ用）。
        例: "sk-...abc123"
        """
        pass
    
    def send_function_response(
        self,
        model_id: str,
        history: Dict,
        function_response_parts: List,
        system_prompt: Optional[str] = None,
        tools: Optional[List] = None,
        **kwargs
    ) -> Any:
        """
        Function Callの結果をAIに送信して応答を得る。
        
        Args:
            model_id: モデルID
            history: 会話履歴
            function_response_parts: 関数実行結果
            system_prompt: システムプロンプト
            tools: ツール定義（継続呼び出し用）
        
        Returns:
            Any: AI応答
        """
        pass
```

--------------------------------------------------------------------------------
4. 標準履歴形式（history）
--------------------------------------------------------------------------------

historyは以下の標準形式で渡される。
各クライアントは、この形式をプロバイダー固有の形式に変換すること。

```python
{
    "conversation_id": "uuid-string",
    "title": "会話タイトル",
    "created_at": "2025-01-01T00:00:00.000Z",  # ISO 8601形式
    "updated_at": "2025-01-01T00:00:00.000Z",
    "model": "gemini-2.5-flash",
    "platform": "gemini",
    
    "messages": {
        # メッセージID -> メッセージ本体のマッピング
        "msg_xxxxxxxxxxxx": {
            "id": "msg_xxxxxxxxxxxx",
            "role": "user",  # "user" | "assistant" | "tool" | "system"
            "content": "こんにちは",
            "parent_id": null,  # ルートメッセージの場合はnull
            "children_ids": ["msg_yyyyyyyyyyyy"],
            "created_at": "2025-01-01T00:00:00.000Z",
            
            # オプションフィールド
            "attachments": [
                {
                    "type": "image",  # "image" | "video" | "audio" | "file"
                    "mime_type": "image/png",
                    "url": "/path/to/file",
                    "name": "screenshot.png"
                }
            ],
            
            # assistantメッセージのみ
            "tool_calls": [
                {
                    "tool_call_id": "call_xxxx",
                    "function_name": "web_search",
                    "arguments": {"query": "検索クエリ"}
                }
            ],
            
            # toolメッセージのみ
            "tool_call_id": "call_xxxx",
            
            # 中断されたメッセージ
            "status": "aborted"
        }
    },
    
    "root_id": "msg_xxxxxxxxxxxx",  # 最初のメッセージのID
    "current_node": "msg_zzzzzzzzzzzz"  # 現在のメッセージのID
}
```

【ロール説明】
- user: ユーザーからのメッセージ
- assistant: AIからの応答
- tool: ツール実行結果
- system: システムメッセージ（通常は履歴に含まれない）

【会話スレッドの取得】
current_nodeからparent_idを辿ることで、現在の会話スレッドを取得できる。
chat_manager.get_conversation_thread(history) を使用すること。

--------------------------------------------------------------------------------
5. ストリーミングイベント形式
--------------------------------------------------------------------------------

send_request_stream() および handle_function_calls_stream() が
yieldするイベントオブジェクトの形式:

```python
# テキストチャンク
{
    "type": "text_chunk",
    "text": "生成されたテキストの断片",
    "is_follow_up": False  # Function Call後の応答の場合True
}

# 思考チャンク（推論モデルのみ）
{
    "type": "thinking_chunk",
    "text": "思考プロセスの断片"
}

# Function Call開始
{
    "type": "function_call_start",
    "function_name": "web_search",
    "tool_name": "Web検索",
    "args": {"query": "検索クエリ"},
    "ui_info": {  # オプション
        "has_ui": True,
        "html_file": "index.html"
    }
}

# ツール進捗
{
    "type": "tool_progress",
    "tool": "web_search",
    "message": "検索中..."
}

# Function Call実行開始
{
    "type": "function_execution_start",
    "count": 2  # 実行するツールの数
}

# Function Call実行完了
{
    "type": "function_execution_complete",
    "execution": {
        "function_name": "web_search",
        "tool_name": "Web検索",
        "args": {"query": "検索クエリ"},
        "result": {"success": True, "data": [...]},
        "has_ui": False,
        "ui_info": None
    }
}

# 関数結果送信中
{
    "type": "sending_function_response"
}

# 完了
{
    "type": "complete",
    "text": "完全な応答テキスト"
}

# 中断
{
    "type": "aborted",
    "text": "中断時点までのテキスト",
    "reason": "user_abort"
}

# エラー
{
    "type": "error",
    "error": "エラーメッセージ"
}
```

--------------------------------------------------------------------------------
6. 実行コンテキスト（context）
--------------------------------------------------------------------------------

handle_function_calls() および handle_function_calls_stream() に
渡されるcontextオブジェクト:

```python
{
    # 基本情報
    "model": "gemini-2.5-flash",
    "thinking_budget": 8192,
    "chat_path": "/path/to/chat/folder",
    "history_path": "/path/to/history.json",
    
    # アプリケーション情報
    "app_path": "/path/to/app.py",
    "main_port": "5000",
    
    # 実行制御
    "abort_event": threading.Event(),  # 中断シグナル
    "execution_id": "uuid-string",      # 実行ID
    
    # チャット操作
    "chat_id": "chat-uuid",
    "chat_manager": ChatManagerInstance,
    
    # ツール操作
    "tool_loader": ToolLoaderInstance,
    
    # デバッグ
    "debug_mode": False,
    
    # システムプロンプト
    "system_prompt": "..."
}
```

--------------------------------------------------------------------------------
7. モデルプロファイル仕様 (ai_profile/*.json)
--------------------------------------------------------------------------------

各モデルのプロファイルは以下の形式のJSONファイルとして作成:

```json
{
    "basic_info": {
        "id": "gemini-2.5-flash",
        "name": "Gemini 2.5 Flash",
        "description": "高速で効率的な汎用モデル",
        "provider": "gemini"
    },
    "capabilities": {
        "context_length": 1048576,
        "max_completion_tokens": 8192,
        "supported_parameters": [
            "temperature",
            "top_p",
            "top_k",
            "max_output_tokens",
            "stop_sequences"
        ]
    },
    "features": {
        "supports_function_calling": true,
        "supports_streaming": true,
        "is_multimodal": true,
        "input_modalities": ["text", "image", "video", "audio"],
        "output_modalities": ["text"],
        "supports_reasoning": false
    },
    "pricing": {
        "input_per_1m_tokens": 0.075,
        "output_per_1m_tokens": 0.30,
        "currency": "USD"
    },
    "limitations": {
        "rate_limit_rpm": 1000,
        "rate_limit_tpm": 4000000
    },
    "notes": {
        "best_for": ["一般的な質問応答", "コード生成", "要約"],
        "release_date": "2025-01-01"
    }
}
```

【必須セクション】
- basic_info
  - id: モデルID（ファイル名と一致させる）
  - name: 表示名
  - description: 説明
  - provider: プロバイダー名

- capabilities
  - context_length: 最大コンテキスト長（トークン）
  - max_completion_tokens: 最大出力トークン
  - supported_parameters: サポートするパラメータのリスト

- features
  - supports_function_calling: Function Calling対応
  - supports_streaming: ストリーミング対応
  - is_multimodal: マルチモーダル対応
  - input_modalities: 入力モダリティ（text, image, video, audio）
  - output_modalities: 出力モダリティ（text, image, audio）
  - supports_reasoning: 推論/思考機能対応

【オプションセクション】
- pricing: 料金情報
- limitations: レート制限等
- notes: その他のメモ

--------------------------------------------------------------------------------
8. 環境変数命名規則
--------------------------------------------------------------------------------

【APIキー】
環境変数名: [PROVIDER_NAME]_API_KEY（大文字、アンダースコア区切り）

例:
- GEMINI_API_KEY
- OPENAI_API_KEY
- ANTHROPIC_API_KEY
- AZURE_OPENAI_API_KEY

【その他の設定】（オプション）
- [PROVIDER_NAME]_API_BASE: カスタムAPIエンドポイント
- [PROVIDER_NAME]_API_VERSION: APIバージョン
- [PROVIDER_NAME]_ORG_ID: 組織ID

--------------------------------------------------------------------------------
9. エラーハンドリング
--------------------------------------------------------------------------------

【推奨されるエラークラス】
プロバイダー固有の例外をキャッチし、以下のパターンで処理すること:

```python
try:
    response = self.client.generate(...)
except AuthenticationError:
    raise ValueError("APIキーが無効です。環境変数を確認してください。")
except RateLimitError as e:
    raise RuntimeError(f"レート制限に達しました。{e.retry_after}秒後に再試行してください。")
except APIError as e:
    raise RuntimeError(f"API呼び出しエラー: {e.message}")
except Exception as e:
    raise RuntimeError(f"予期しないエラー: {str(e)}")
```

【リトライ対応】
一時的なエラー（503, 429等）の場合は、呼び出し元でリトライできるよう
エラーメッセージに情報を含めること。

--------------------------------------------------------------------------------
10. 実装例（スケルトン）
--------------------------------------------------------------------------------

```python
# ai_client/example/example_client.py

import os
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional
from threading import Event


class ExampleClient:
    """
    Example AI Provider クライアント
    """
    
    @staticmethod
    def get_provider_name() -> str:
        return "example"
    
    @staticmethod
    def get_profile_dir() -> Path:
        return Path(__file__).parent / "ai_profile"
    
    def __init__(self):
        self.api_key = os.environ.get("EXAMPLE_API_KEY")
        if not self.api_key:
            raise ValueError(
                "EXAMPLE_API_KEY が設定されていません。"
                "環境変数を設定してください。"
            )
        
        # クライアントライブラリの初期化
        # self.client = ExampleSDK(api_key=self.api_key)
    
    def send_request(
        self,
        model_id: str,
        history: Dict,
        current_text_input: str,
        current_file_paths: List[str],
        temperature: float = 0.7,
        thinking_budget: Optional[int] = None,
        tools: Optional[List] = None,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> Any:
        # 1. 標準履歴をプロバイダー形式に変換
        messages = self._convert_history(history)
        
        # 2. 現在の入力を追加
        messages.append({"role": "user", "content": current_text_input})
        
        # 3. ファイルを処理
        if current_file_paths:
            # ファイル処理ロジック
            pass
        
        # 4. API呼び出し
        response = self.client.chat.create(
            model=model_id,
            messages=messages,
            temperature=temperature,
            tools=tools,
            system=system_prompt
        )
        
        return response
    
    def send_request_stream(
        self,
        model_id: str,
        history: Dict,
        current_text_input: str,
        current_file_paths: List[str],
        temperature: float = 0.7,
        thinking_budget: Optional[int] = None,
        tools: Optional[List] = None,
        abort_signal: Optional[Event] = None,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> Iterator:
        # ストリーミング実装
        for chunk in self.client.chat.create_stream(...):
            # 中断チェック
            if abort_signal and abort_signal.is_set():
                yield {"type": "aborted", "text": accumulated_text}
                return
            
            yield {"type": "text_chunk", "text": chunk.text}
        
        yield {"type": "complete", "text": full_text}
    
    def extract_response_text(self, response: Any) -> str:
        return response.content[0].text
    
    def handle_function_calls(
        self,
        response: Any,
        model_id: str,
        history: Dict,
        context: Dict
    ) -> tuple:
        # Function Call処理
        tool_loader = context.get("tool_loader")
        executions = []
        
        for tool_call in response.tool_calls:
            result = tool_loader.execute_tool(
                tool_call.function.name,
                tool_call.function.arguments,
                context
            )
            executions.append(result)
        
        # 結果を送信して最終応答を取得
        final_response = self.send_function_response(...)
        
        return final_response, executions
    
    def handle_function_calls_stream(
        self,
        stream_response: Iterator,
        model_id: str,
        history: Dict,
        context: Dict
    ) -> Iterator:
        # ストリーミング版Function Call処理
        for event in stream_response:
            yield event
    
    def _convert_history(self, history: Dict) -> List[Dict]:
        """標準履歴をプロバイダー形式に変換"""
        messages = []
        # 変換ロジック
        return messages
```

--------------------------------------------------------------------------------
11. チェックリスト
--------------------------------------------------------------------------------

新しいAIクライアントを実装する際は、以下を確認すること:

□ フォルダ名が小文字英数字とアンダースコアのみ
□ [provider_name]_client.py ファイルが存在する
□ クラス名が [ProviderName]Client である
□ get_provider_name() がフォルダ名を返す
□ get_profile_dir() が ai_profile/ を返す
□ requirements.txt に依存ライブラリを記載
□ ai_profile/ に少なくとも1つのモデルプロファイルがある
□ 必須メソッドがすべて実装されている
□ 環境変数名が [PROVIDER_NAME]_API_KEY 形式
□ エラーハンドリングが適切に実装されている
□ ストリーミングで abort_signal をチェックしている
□ 標準履歴形式を正しく変換している

================================================================================
```